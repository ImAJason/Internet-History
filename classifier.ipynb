{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import sqlite3\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import metrics,preprocessing,cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_clean():\n",
    "    \n",
    "    ## ==== Clean data ==== ##\n",
    "    \n",
    "    URLS, fulltime, date, time, duration = pull_history(fakehistory)\n",
    "    df=pd.DataFrame(URLS,columns=['URL'])\n",
    "    df['time'] = time\n",
    "    df['freq'] = df.groupby('URL')['URL'].transform('count')\n",
    "    df['duration'] = duration\n",
    "\n",
    "    ##Only considering top 16 visited sites for now\n",
    "    ##Get rid of any rows with time duration of 0\n",
    "\n",
    "    filt = []\n",
    "    for i in df.freq:\n",
    "        filt.append(i)\n",
    "        \n",
    "    filt = list(set(filt))\n",
    "    filt.sort(reverse = True)\n",
    "    \n",
    "    df = df[df.duration != 0]\n",
    "    df = df[df.freq > np.amin(filt[0:18])]\n",
    "    df = df[['URL', 'time']]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_prep(cleandf):\n",
    "    \n",
    "    ## ==== Prep data for modeling ==== ##\n",
    "    \n",
    "    sites = list(cleandf.URL)\n",
    "    unique_sites = list(set(cleandf.URL))\n",
    "    train_set = {}\n",
    "    train_inverse_set = {}\n",
    "    count = 0\n",
    "    for i in unique_sites:\n",
    "        count += 1\n",
    "        train_set[i] = count\n",
    "        train_inverse_set[count] = i\n",
    "        \n",
    "    X = np.array(list(cleandf.time))\n",
    "    X = train_x.reshape((len(train_x),1))\n",
    "    y = [train_set[i] for i in sites]\n",
    "    \n",
    "    return X, y, train_inverse_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For randomforest: Best C = 170.000000; F-score = 0.628359\n",
      "For gradientboost: Best C = 10.000000; F-score = 0.682377\n"
     ]
    }
   ],
   "source": [
    "SEED = 23\n",
    "bestmodel = {}\n",
    "\n",
    "## ==== Training & Metrics ==== ##\n",
    "\n",
    "cleandf = data_clean()\n",
    "X, y, train_inverse_set = data_prep(cleandf)\n",
    "\n",
    "n = 10  #repeat 10 times for more precise results\n",
    "for i in range(n):\n",
    "    \n",
    "    #split dataset into training, validation, testing with ratio of 3:1:1\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "        X, y, test_size=.4, random_state=i*SEED)\n",
    "    \n",
    "    X_test, X_val, y_test, y_val = cross_validation.train_test_split(\n",
    "        X_test, y_test, test_size=.5, random_state=i*SEED)\n",
    "\n",
    "################################################################################\n",
    "## The following tests potential models via cross validation \n",
    "## Want to find the optimal values for the tuning parameters\n",
    "## Returns parameter with best performance(using a F score as the cross-validation metric)\n",
    "\n",
    "#** Used F-score because it has a nice balance between precision and recall in multiclass classification\n",
    "        \n",
    "# pick model\n",
    "modelname = \"randomforest\"\n",
    "\n",
    "################################################################################\n",
    "\n",
    "if modelname == \"randomforest\":\n",
    "    C = np.linspace(10, 300, num = 30)\n",
    "    models = [RandomForestClassifier(n_estimators = int(c)) for c in C]\n",
    "\n",
    "if modelname == \"gradientboost\":\n",
    "    C = np.linspace(10, 300, num = 30)\n",
    "    models = [GradientBoostingClassifier(n_estimators = int(c)) for c in C]\n",
    "    \n",
    "#### Testing other models in the future\n",
    "\n",
    "## calculate scores \n",
    "cv_scores = [0] * len(models)\n",
    "for i, model in enumerate(models):\n",
    "    cv_scores[i] = np.mean(cross_validation.cross_val_score(\n",
    "            model, X_val, y_val, cv=5, scoring='f1_weighted'))\n",
    "\n",
    "best = cv_scores.index(max(cv_scores))\n",
    "best_c = C[best]\n",
    "best_cv = cv_scores[best]\n",
    "\n",
    "bestmodel[modelname] = [best_c, best_cv]\n",
    "\n",
    "for k,v in bestmodel.items():\n",
    "    print \"For %s: Best C = %f; F-score = %f\" %(k, v[0], v[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "#### F-scores were not very good.\n",
    "Went ahead and used best model(gradient boosting) for now.\n",
    "\n",
    "(ideas for improving: combining models, one-hot encoding with a different model/approach?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.686194393123\n"
     ]
    }
   ],
   "source": [
    "## ==== Training & Metrics cont'd ==== ##\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators = 10)\n",
    "eval_model = gb.fit(X_train,y_train)\n",
    "eval_predy = eval_model.predict(X_test)\n",
    "\n",
    "# Compute F-score metric\n",
    "print f1_score(y_test, eval_predy, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended site at 18.49: www.reddit.com\n"
     ]
    }
   ],
   "source": [
    "## ==== Predictions ==== ##\n",
    "\n",
    "#get current time\n",
    "time_now = str(time.strftime('%H:%M'))\n",
    "adjusted_time_now = [adjusted_time(time.strftime('%H:%M'))]\n",
    "\n",
    "#retrained the model on the whole dataset when making actual prediction\n",
    "model_all = gb.fit(X,y)\n",
    "prediction = model_all.predict(adjusted_time_now)\n",
    "\n",
    "for p in prediction:\n",
    "    print \"Recommended site at %s: %s\" %(adjusted_time_now, train_inverse_set[p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "####Related work for future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most visited site on today(Monday) is: youtube.com\n"
     ]
    }
   ],
   "source": [
    "## === Return the most frequently visited website on a specific day. === ##  \n",
    "\n",
    "df=pd.DataFrame(URLS,columns=['URL'])\n",
    "df['freq'] = df.groupby('URL')['URL'].transform('count')\n",
    "df['dates'] = dates\n",
    "df =  df.sort('freq', ascending = False)\n",
    "df = df.reset_index(drop=True) \n",
    "mode = lambda x: x.mode() if len(x) > 2 else np.array(x)\n",
    "df = df.groupby('dates')['URL'].agg(mode)\n",
    "days = dict(df)\n",
    "\n",
    "today = str(time.strftime(\"%A\"))\n",
    "print \"Most visited site on today(%s) is: %s\" %(today, days[today])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## === One-hot encoding === ##\n",
    "#encode the category IDs\n",
    "\n",
    "X, prep_y, train_inverse_set = data_prep(cleandf)\n",
    "\n",
    "y = [[prep_y[i]] for i in xrange(len(prep_y))]\n",
    "\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
